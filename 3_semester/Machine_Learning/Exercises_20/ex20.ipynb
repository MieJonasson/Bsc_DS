{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises 20 \n",
    "*on Convolutional Neural Networks (CNN)*\n",
    "\n",
    "At this point math is too complicated, so we use pytorch / keras already implemented solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "- 32 x 32 picture, \n",
    "- 5 x 5 filter, \n",
    "- padding 0, \n",
    "- stride 1\n",
    "\n",
    "**Answers**:\n",
    "- New Image; 28 x 28; (32 - (5 - 1)) = 28 (for each filter applied seperately)\n",
    "- To avoid reduction; 2 rows of padding! \n",
    "- Number of parameters; 25 per filter (5x5) + 1 bias per filter = 78 parameters (including biases)\n",
    "- Constrain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "- 200 x 300 picture\n",
    "- 100 Feature maps\n",
    "- 3 x 3 Kernel in 3 layers\n",
    "- stride 2\n",
    "- padding \"same\" (i.e. 1 row here - whatever is needed to keep the output dimension the same)\n",
    "\n",
    "This means;\n",
    "Input n x 3 x 200 x 300\n",
    "C1 n x 100 x 200 x 300\n",
    "C2 n x 200 x 200 x 300\n",
    "C3 n x 400 x 200 x 300\n",
    "\n",
    "**Answers**:\n",
    "- Parameters \n",
    "  - **C1**: One filter is 3 x 3 (x 3 dimensions RGB)(x 100 feature maps) + 100 biases = 2.800\n",
    "  - **C2**: 3 x 3 x 100 x 200 + 200 = 180.200\n",
    "  - **C3**: 3 x 3 x 200 x 400 + 400 = 720.400\n",
    "  - **Total**: 903.400\n",
    "- 903.400 * 32 (bits) / 8 (bytes) / 1000 (kB) / 1000 (MB) = 6.49 MB (Just for storing parameters for 1 image)\n",
    "- 50 x 200 x 300 x 3 x 32 / 8 / 1000 / 1000 = 36 MB For storing 50 Images :)\n",
    "- Max pooling has WAY less parameters & Compresses features to be less volatile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class Data that holds fashion mnist data\n",
    "## Default 60.000 x 1 x 28 x 28 Train & 10.000 x 1 x 28 x 28 Test\n",
    "class Data(Dataset):\n",
    "    def __init__(self, n, train = True):\n",
    "        # Load N fashion mnist images + targets\n",
    "        x = [] # Images (n x 784)\n",
    "        y = [] # Class labels\n",
    "        with open(f'data/fashion-mnist_{\"train\" if train else \"test\"}.csv', 'r') as f:\n",
    "            f.readline() # Header, w. column names\n",
    "            for i in range(n):\n",
    "                line = f.readline()\n",
    "                sample = list(map(int, line.strip().split(',')))\n",
    "                trg = sample[0]\n",
    "                img = sample[1:]\n",
    "                x.append(img)\n",
    "                y.append(trg)\n",
    "        \n",
    "        # Reshape and convert to tensors\n",
    "        self.x = torch.tensor(x).reshape(n, 1, 28, 28).float()\n",
    "        self.y = torch.tensor(y).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonas\\AppData\\Local\\Temp\\ipykernel_17624\\584436476.py:19: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:77.)\n",
      "  self.x = torch.tensor(x).reshape(n, 1, 28, 28).float()\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "train = Data(n=10**3, train=True)\n",
    "test = Data(n=10**2, train=False)\n",
    "\n",
    "# Initialise data loader\n",
    "batchtrain = DataLoader(train, batch_size=len(train))\n",
    "minibatchtrain = DataLoader(train, batch_size=2**7, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding - Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet CNN\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Inherited constructor\n",
    "        super().__init__()\n",
    "\n",
    "        # Input dimension: n (sample) x 1 (input channels) x 28 x 28 (image dimensions)\n",
    "        self.c1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2) # n x 6 x 28 x 28\n",
    "        self.s2 = nn.AvgPool2d(kernel_size=2, stride=2) # n x 6 x 14 x 14\n",
    "\n",
    "        self.c3 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5) # n x 16 x 10 x 10\n",
    "        self.s4 = nn.AvgPool2d(kernel_size=2, stride=2) # n x 16 x 5 x 5\n",
    "\n",
    "        self.c5 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5) # n x 120 x 1 x 1 (i.e. n x 120)\n",
    "\n",
    "        self.f6 = nn.Linear(in_features=120, out_features=84) # n x 84\n",
    "        self.out = nn.Linear(in_features=84, out_features=10)\n",
    "\n",
    "        # Activation Function\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First Comvolutional layer + avg. pooling\n",
    "        x = self.tanh(self.c1(x))\n",
    "        x = self.tanh(self.s2(x))\n",
    "\n",
    "        x = self.tanh(self.c3(x))\n",
    "        x = self.tanh(self.s4(x))\n",
    "\n",
    "        x = self.tanh(self.c5(x)).reshape(len(x), -1)\n",
    "\n",
    "        x = self.tanh(self.f6(x))\n",
    "\n",
    "        x = self.softmax(self.out(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        torch.argmax(self.forward(x), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "img, trg = next(iter(minibatchtrain))\n",
    "print(img.shape, trg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model\n",
    "lenet = LeNet()\n",
    "\n",
    "# Optimizer & Cost function\n",
    "optimiser = torch.optim.Adam(lenet.parameters(), lr=10e-3)\n",
    "cost = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 2.0980172157287598 Train Accuracy: 0.0\n",
      "Epoch 2 Loss: 1.9181586503982544 Train Accuracy: 0.0\n",
      "Epoch 3 Loss: 1.797006368637085 Train Accuracy: 0.0\n",
      "Epoch 4 Loss: 1.8298213481903076 Train Accuracy: 0.0\n",
      "Epoch 5 Loss: 1.7904880046844482 Train Accuracy: 0.0\n",
      "Epoch 6 Loss: 1.7941962480545044 Train Accuracy: 0.0\n",
      "Epoch 7 Loss: 1.6947968006134033 Train Accuracy: 0.0\n",
      "Epoch 8 Loss: 1.6915745735168457 Train Accuracy: 0.0\n",
      "Epoch 9 Loss: 1.7028948068618774 Train Accuracy: 0.0\n",
      "Epoch 10 Loss: 1.6342530250549316 Train Accuracy: 0.0\n",
      "Epoch 11 Loss: 1.7676767110824585 Train Accuracy: 0.0\n",
      "Epoch 12 Loss: 1.7009880542755127 Train Accuracy: 0.0\n",
      "Epoch 13 Loss: 1.7352887392044067 Train Accuracy: 0.0\n",
      "Epoch 14 Loss: 1.645838975906372 Train Accuracy: 0.0\n",
      "Epoch 15 Loss: 1.7700735330581665 Train Accuracy: 0.0\n",
      "Epoch 16 Loss: 1.6923681497573853 Train Accuracy: 0.0\n",
      "Epoch 17 Loss: 1.7134931087493896 Train Accuracy: 0.0\n",
      "Epoch 18 Loss: 1.6652005910873413 Train Accuracy: 0.0\n",
      "Epoch 19 Loss: 1.6950690746307373 Train Accuracy: 0.0\n",
      "Epoch 20 Loss: 1.6918050050735474 Train Accuracy: 0.0\n",
      "Epoch 21 Loss: 1.711517333984375 Train Accuracy: 0.0\n",
      "Epoch 22 Loss: 1.7534734010696411 Train Accuracy: 0.0\n",
      "Epoch 23 Loss: 1.7625077962875366 Train Accuracy: 0.0\n",
      "Epoch 24 Loss: 1.7120177745819092 Train Accuracy: 0.0\n",
      "Epoch 25 Loss: 1.7189208269119263 Train Accuracy: 0.0\n",
      "Epoch 26 Loss: 1.618939757347107 Train Accuracy: 0.0\n",
      "Epoch 27 Loss: 1.6907070875167847 Train Accuracy: 0.0\n",
      "Epoch 28 Loss: 1.6941553354263306 Train Accuracy: 0.0\n",
      "Epoch 29 Loss: 1.7046542167663574 Train Accuracy: 0.0\n",
      "Epoch 30 Loss: 1.6944210529327393 Train Accuracy: 0.0\n",
      "Epoch 31 Loss: 1.6644953489303589 Train Accuracy: 0.0\n",
      "Epoch 32 Loss: 1.674561619758606 Train Accuracy: 0.0\n",
      "Epoch 33 Loss: 1.8021752834320068 Train Accuracy: 0.0\n",
      "Epoch 34 Loss: 1.8237450122833252 Train Accuracy: 0.0\n",
      "Epoch 35 Loss: 1.7377187013626099 Train Accuracy: 0.0\n",
      "Epoch 36 Loss: 1.7274998426437378 Train Accuracy: 0.0\n",
      "Epoch 37 Loss: 1.6799960136413574 Train Accuracy: 0.0\n",
      "Epoch 38 Loss: 1.6387711763381958 Train Accuracy: 0.0\n",
      "Epoch 39 Loss: 1.7045056819915771 Train Accuracy: 0.0\n",
      "Epoch 40 Loss: 1.6784756183624268 Train Accuracy: 0.0\n",
      "Epoch 41 Loss: 1.650745153427124 Train Accuracy: 0.0\n",
      "Epoch 42 Loss: 1.7258375883102417 Train Accuracy: 0.0\n",
      "Epoch 43 Loss: 1.67593514919281 Train Accuracy: 0.0\n",
      "Epoch 44 Loss: 1.6745058298110962 Train Accuracy: 0.0\n",
      "Epoch 45 Loss: 1.6970152854919434 Train Accuracy: 0.0\n",
      "Epoch 46 Loss: 1.645875096321106 Train Accuracy: 0.0\n",
      "Epoch 47 Loss: 1.7914477586746216 Train Accuracy: 0.0\n",
      "Epoch 48 Loss: 1.6893490552902222 Train Accuracy: 0.0\n",
      "Epoch 49 Loss: 1.751421570777893 Train Accuracy: 0.0\n",
      "Epoch 50 Loss: 1.6947128772735596 Train Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "acc = lambda y, yhat: (y == yhat) / len(y)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xbatch, ybatch in minibatchtrain:\n",
    "        # zero out gradients\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Make predictions\n",
    "        pred = lenet(xbatch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = cost(pred, ybatch)\n",
    "\n",
    "        # Backpropagate loss\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "    \n",
    "    xtrain, ytrain = next(iter(batchtrain))\n",
    "    ypred = lenet.predict(xtrain)\n",
    "    print(f\"Epoch {epoch+1} Loss: {loss} Train Accuracy: {acc(ytrain,ypred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "059b92ebffe316512df3810dcdd9739bd5d694b60baa1e9e8136193b1cf34557"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
