{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40233bd4",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:40px;\"><center>Support Vector Machines (SVM)\n",
    "   <br>\n",
    "</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfa34e1",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "# Summary\n",
    "\n",
    "These ecxercises aim a deeper understanding of how a hard-margin support vector machine works. The first two exercises are conceptual and can be done by hand. The last exercise is a coding exercise and has several steps that you take to implement a hard-margin SVM from scratch (mostly)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02263903",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "This exercise is about hyperplanes in two dimensions. Imagine we have a 2-dimensional feature space with features X1 and X2.\n",
    "- Sketch the hyperplane $1 + 3X1 − X2 = 0$. Indicate the set of points for which $1 + 3X1 − X2 > 0$, as well as the set of points for which $1 + 3X1 − X2 < 0$.\n",
    "\n",
    "- On the same plot, sketch the hyperplane $−2 + X1 + 2X2 = 0$. Indicate the set of points for which $−2 + X1 + 2X2 > 0$, as well as the set of points for which $−2 + X1 + 2X2 < 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60adb8f6",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "Here we explore hard-margin SVM (maximal margin classifier) on a toy data.\n",
    "We are given $n = 7$ observations in $p = 2$ dimensions. For each observation, there is an associated class label.\n",
    "\n",
    "| Obs. | $X_1$ | $X_2$ | $Y$ \n",
    "| :- | :- | :- | :- \n",
    "| 1 | 3 | 4 | Red \n",
    "| 2 | 2 | 2 | Red  \n",
    "| 3 | 4 | 4 | Red  \n",
    "| 4 | 1 | 4 | Red \n",
    "| 5 | 2 | 1 | Blue  \n",
    "| 6 | 4 | 3 | Blue \n",
    "| 7 | 4 | 1 | Blue  \n",
    "\n",
    "- Sketch the observations\n",
    "\n",
    "- Sketch a separating hyperplane. \n",
    "\n",
    "- Sketch the optimal separating hyperplane, and provide the equation for this hyperplane.\n",
    "\n",
    "- Describe the classification rule, something along the lines of \"Classify to Red if $\\beta_0 +\\beta_1 X1 +\\beta_2 X2 > 0$, and classify to Blue otherwise\". \n",
    "\n",
    "- On your sketch, indicate the margin for the maximal margin hyperplane.\n",
    "\n",
    "- Indicate the support vectors for your classifier.\n",
    "\n",
    "- Argue that a slight movement of the seventh observation would not affect the maximal margin hyperplane.\n",
    "\n",
    "- Sketch a hyperplane that is not the optimal separating hyperplane, and provide the equation for this hyperplane.\n",
    "\n",
    "- Draw an additional observation on the plot so that the two classes are no longer separable by a hyperplane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f557ffc",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "## Hard-margin SVM\n",
    "\n",
    "In the lecture we talked about the objective function for the hard-margin SVM:\n",
    "\n",
    "$$\\underset{\\mathrm w, b} {\\arg\\min} \\frac{1}{2} \\lVert  \\mathrm w \\rVert^2$$\n",
    "\n",
    "$$\\text{subject to  } y(\\mathrm{w \\cdot x + b}) -1 \\ge 0 $$\n",
    "\n",
    "a) In your group, discuss what this objective function means and how it is driven.\n",
    "\n",
    "b) What is the assumption about the data in hard-margin SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cbbd86",
   "metadata": {},
   "source": [
    "c) Do you think SVM is sensitive to scales of the different features (do we need to rescale our data)? Disucss the answer in your group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e53217",
   "metadata": {},
   "source": [
    "## Implementing hard-margin SVM\n",
    "\n",
    "Normally, one uses available implementations of SVM from various libraries. \n",
    "But here we try to implement it ourselves, to get a deeper understanding of how it works.\n",
    "\n",
    "The goal is to implement hard-margin SVM. \n",
    "\n",
    "We start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9eb5a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f658f2db",
   "metadata": {},
   "source": [
    "Then we generate our random dataset; this will be some 2D data that is linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a115abae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to make things repeatable\n",
    "np.random.seed(1)\n",
    "\n",
    "N = 20\n",
    "X = np.array(list(zip(np.random.normal(size=N), np.random.normal(size=N))))  # standard Gaussian samples in R^2\n",
    "c0 = np.where(X[:,0] < 0)  # indices of points (x,y) for which XOR(x,y) is true, i.e. (x<0&y>0)|(x>0&y<0)\n",
    "c1 = np.where(X[:,0] > 0)  # indices of data points belonging to the other class\n",
    "y = np.ones(N)  # labels\n",
    "y[c1] = -1  # negative class labels\n",
    "X[c1,0] = X[c1,0] + 0.1  # make a little gap between the two groups \n",
    "\n",
    "\n",
    "# Rotation\n",
    "theta = np.radians(45)\n",
    "c, s = np.cos(theta), np.sin(theta)\n",
    "R = np.array([[c,-s],[s,c]]) # rotation matrix\n",
    "X =  X @ R\n",
    "\n",
    "# plots\n",
    "# plot training data points\n",
    "for cls, clr in zip((c0, c1), ('r', 'b')):\n",
    "    plt.scatter(X[cls, 0], X[cls, 1], color=clr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772df720",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Next we set up the SVM model. \n",
    "\n",
    "In the course, we learned to rewrite the hard-margin SVM objective function as its dual representation (using Lagrangian multipliers), and maximize it.\n",
    "\n",
    "$$\\arg\\max_{\\alpha} \\tilde{L}(\\alpha), \\quad\\quad \\tilde{L}(\\alpha) = \\sum_i^n \\alpha_i - \\frac{1}{2} \\sum_i^n \\sum_j^n \\alpha_i \\alpha_j y_i y_j (\\mathbf{x_i} \\cdot \\mathbf{x_j})$$\n",
    "$$\\text{subject to }\\quad \\alpha \\geq 0 \\quad{} \\text{and} \\quad{} \\sum_i^n \\alpha_i y_i = 0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e8501d",
   "metadata": {},
   "source": [
    "however, for coding that, it is easier to work with matrices. \n",
    "\n",
    "Therefore, we define the matrix $\\mathbf{A} = \\pmb{\\alpha}^\\intercal \\pmb{\\alpha} \\circ \\mathbf{y} \\mathbf{y}^\\intercal \\circ K$, \n",
    " - $K$ is an $N \\times N$ matrix resulted from the dot product between every two data point $\\mathbf{x}$, that is $K_{i,j}=\\mathbf{x_i}.\\mathbf{x_j}$\n",
    "\n",
    " - $\\pmb{\\alpha}$ is a $1\\times N$ matrix in this equation, so $\\pmb{\\alpha}^\\intercal \\pmb{\\alpha}$ is an $N \\times N$-matrix\n",
    "\n",
    " - $\\mathbf{y}$ is a $N \\times 1$ matrix in this equation, so $\\mathbf{y} \\mathbf{y}^\\intercal$ is an $N \\times N$-matrix\n",
    "\n",
    " - The notation $\\circ$ in the equation stands for the element-wise product (Hadamard product). \n",
    "\n",
    "\n",
    "We can sum up all the elements of $\\mathbf{A}$ to get the double-sum for $\\tilde{L}(\\alpha)$, as \n",
    "\n",
    "$$\n",
    "\\tilde{L}(\\alpha) = \\sum_{i} \\alpha_i - \\frac{1}{2} \\sum_{i,j} A_{i,j}, \\quad\n",
    "\\text{where $A_{i,j}$ is the value at the $i$th row and $j$th column of $\\mathbf{A}$.}\n",
    "$$\n",
    "\n",
    "Eventualy, we need to maximze the $\\tilde{L}(\\alpha)$ with respect to $\\alpha$ and then use the solutions of $\\alpha$ for prediction. \n",
    "\n",
    "We will do that in separates steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65c6fd9",
   "metadata": {},
   "source": [
    "\n",
    "### Let's first compute the matrix $K$\n",
    "### *(a) implement the matrix computation*\n",
    "complete the code for the function \"gram\" below that computes the matrix $K$.\n",
    "As inputs, the function gets a data matrix $X$ and a kernel function k (in the code below it is implemented as linear_kernel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3bead4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(x,y):\n",
    "    return x @ y\n",
    "\n",
    "def gram(X, k):\n",
    "    \"\"\"compute the Gram matrix, given a data matrix X and kernel k; N^2 time complexity\"\"\"\n",
    "    # Your code here\n",
    "    \n",
    "\n",
    "    return K\n",
    "\n",
    "kernel = lambda x, y: linear_kernel(x,y)\n",
    "K = gram(X, kernel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c6a23",
   "metadata": {},
   "source": [
    "### Optimising the Quadratic Program (QP) - $\\tilde{L}(\\alpha)$\n",
    "The equation for $\\tilde{L}(\\alpha)$ and its constraints represent a QP\n",
    "and therefore we can optimize it by a QP solver, in scipy library.\n",
    "The library has a function ***optimize.minimize(...)***  (there is no optimize.maximize). \n",
    "\n",
    "Therefore we have to negate the $\\tilde{L}(\\alpha)$ and minimize it.\n",
    "$$\\arg\\min_{\\alpha} -(\\sum_{i} \\alpha_i - \\frac{1}{2} \\sum_{i,j} A_{i,j})$$\n",
    "$$\\text{s.t.}\\quad \\alpha \\geq 0 \\quad{} \\text{and} \\quad{} \\sum_i^n \\alpha_i y_i = 0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb89fc58",
   "metadata": {},
   "source": [
    "### *(b) implement the loss function* \n",
    "Now we want to implement $\\tilde{L}(\\alpha)$ as a function called loss.\n",
    "Complete the code. \n",
    "\n",
    "*Note: In python you can use $*$ for Hadamard product (element-wise product)* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a77784d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(a): \n",
    "    \"\"\"Evaluate the negative of the dual function at `a`.\n",
    "    We access the optimization data (Gram matrix K and target vector y) from outer scope for convenience.\n",
    "    :param a: dual variables (alpha)\n",
    "    \"\"\"\n",
    "    a = a.reshape(1,-1)   # We reshape a as we assumed it as a 1 x N matrix in the equations above\n",
    "    yv = y.reshape(-1,1)  # We reshape y as we assumed it as a N x 1 matrix in the equations above\n",
    "    \n",
    "    return # your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e090f7c",
   "metadata": {},
   "source": [
    "To use the QP solver ***optimizer.minimize(..)*** function, we need to pass several items to it. Some of them are:\n",
    "* a loss function to it.\n",
    "* the Jacobian matrix of the loss function (Jacobian matrix is the partial derivatives of the loss function with respect to each element of $\\pmb{\\alpha}$).\n",
    "* constraints: the inequalities, their jacobian, the equalities, their jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a8d898d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jac(a):\n",
    "    \"\"\"Calculate the Jacobian of the loss function (for the QP solver)\"\"\"\n",
    "    a = a.reshape(1,-1)\n",
    "    yv = y.reshape(-1,1)\n",
    "    j = - np.ones_like(a) + a @ ((yv @ yv.T) * K)\n",
    "    return j.flatten()\n",
    "\n",
    "# write the constraints in matrix notation: inequalities should be formulated as f >= 0 , equalities as f = 0\n",
    "# We have one equality constraint: a @ y.T = 0\n",
    "# We have N inequalities (one for each a_i), therefore: A @ a >= 0,   \n",
    "A = np.eye(N)   \n",
    "\n",
    "constraints = ({'type': 'ineq', 'fun': lambda a: A @ a, 'jac': lambda a: A},\n",
    "               {'type': 'eq', 'fun': lambda a: a @ y.T, 'jac': lambda a: y.T})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68735475",
   "metadata": {},
   "source": [
    "To complete the optimization, we also need to pass initial guess for $\\alpha$ and the QP optimization method (here we use SLSQP method), as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c346a779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "a0 = np.random.rand(N)  # initial guess\n",
    "print('Initial loss: ' + str(loss(a0)))\n",
    "\n",
    "res = minimize(loss, a0, jac=jac, constraints=constraints, method='SLSQP', options={})\n",
    "\n",
    "print('Optimized loss: ' + str(res.fun))\n",
    "\n",
    "a = res.x  # optimal Lagrange multipliers\n",
    "a[np.isclose(a, 0)] = 0  # zero out value that are nearly zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7b1de8",
   "metadata": {},
   "source": [
    "### Prediction \n",
    "Now that we trained the model and have the $\\pmb{\\alpha}$, we can use it to predict (classify) data points.\n",
    "\n",
    "$$ h(x) = \\sum_i^n \\alpha_i y_i( x_i \\cdot x )+ b $$\n",
    "\n",
    "$$ b = \\frac{1}{N_S}\\sum_{i \\in S}(y_i - \\sum_{j \\in S} \\alpha_jy_j(x_i.x_j)), \\quad \\text{where $S$ is the set of support vectors}$$\n",
    "\n",
    "### *(c) Which data points are the support vectors?* \n",
    "As we can see above, only the support vectors contribute to prediction. \n",
    "Which data points are the support vectors?\n",
    "Complete the code to assign the index of support vectors to variable *support_idx*\n",
    "\n",
    "Then use *support_idx* and the equation above to compute b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1192e81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the intercept term, b\n",
    "support_idx = np.where(a > 0)[0] \n",
    "print('Total %d data points, %d support vectors' % (N, len(support_idx)))\n",
    "X_sv = X[support_idx]\n",
    "y_sv = y[support_idx]\n",
    "a_sv = a[support_idx]\n",
    "\n",
    "def compute_b(X,y,a,k):\n",
    "    # your code here\n",
    "\n",
    "b = compute_b(X_sv,y_sv,a_sv,kernel)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a3e725",
   "metadata": {},
   "source": [
    "### *(d) implement the prediction* \n",
    "Now use the equation above for $h(x)$, to implement the prediction function below. \n",
    "Remember that we only need to pass the support vectors data to the prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9929ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test, X, y, a, b, k):\n",
    "    \"\"\"Form predictions on a test set.\n",
    "    :param test: matrix of data to classify\n",
    "    :param X: matrix of support vector features\n",
    "    :param y: vector of support vectors labels\n",
    "    :param a: optimal dual variables (alpha) for support vectors\n",
    "    :param b: optimal intercept, b\n",
    "    :param k: kernel function\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "\n",
    "predictions = predict(X, X_sv, y_sv, a_sv, b, kernel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c888ea60",
   "metadata": {},
   "source": [
    "### Finaly, lets plot the results!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cbce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training data points\n",
    "for cls, clr in zip((c0, c1), ('r', 'b')):\n",
    "    plt.scatter(X[cls, 0], X[cls, 1], color=clr)\n",
    "# add circles around support vectors\n",
    "plt.scatter(X_sv[:, 0], X_sv[:, 1], color='g', s=100, facecolors='none', edgecolors='g',\n",
    "            label='support vectors')\n",
    "\n",
    "# plot the decision boundary and margins in the input space\n",
    "grid = np.arange(X.min(), X.max(), 0.05)\n",
    "xx, yy = np.meshgrid(grid, grid)\n",
    "zs = predict(np.array(list(zip(np.ravel(xx), np.ravel(yy)))), X, y, a, b, kernel)\n",
    "zz = zs.reshape(xx.shape)\n",
    "CS = plt.contour(xx, yy, zz, levels=[-1, 0, 1], )  # margin, separating hyperplane, margin\n",
    "plt.clabel(CS, fmt='%2.1d', colors='k')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.legend(loc='best')\n",
    "plt.title(\"SVM Classification of Data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
